{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stormfront_db = 'sqlite:////home/sam/data/stormfront/stormfront.sqlite'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_text = pd.read_sql_query('''SELECT p2.thread_url AS thread_url, GROUP_CONCAT(p2.text, \" \") AS text FROM \n",
    "        (SELECT s.*, p.thread_url\n",
    "            FROM (SELECT subposts.post_id, GROUP_CONCAT(subposts.content, \" \") AS text FROM subposts GROUP BY post_id) AS s\n",
    "            INNER JOIN (SELECT thread_url, post_id FROM posts) AS p ON s.post_id = p.post_id)\n",
    "        AS p2 GROUP BY p2.thread_url;''', stormfront_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>thread_url</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.stormfront.org/forum/t1000001</td>\n",
       "      <td>\\n\\nAttacks 83 yr old man - YouTube\\n\\n\\n\\n\\n\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.stormfront.org/forum/t1000015</td>\n",
       "      <td>\\nSource: 'It's making children cry': Police t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.stormfront.org/forum/t1000028</td>\n",
       "      <td>MSNBC?s Matthews: Tea Party Racist by Using W...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.stormfront.org/forum/t1000034</td>\n",
       "      <td>\\n \\r\\n\\t\\t\\t\\r\\n\\t\\t\\tMay we still refer to t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.stormfront.org/forum/t1000039</td>\n",
       "      <td>\\n \\nWe have become the animal farm, and some ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  thread_url  \\\n",
       "0  https://www.stormfront.org/forum/t1000001   \n",
       "1  https://www.stormfront.org/forum/t1000015   \n",
       "2  https://www.stormfront.org/forum/t1000028   \n",
       "3  https://www.stormfront.org/forum/t1000034   \n",
       "4  https://www.stormfront.org/forum/t1000039   \n",
       "\n",
       "                                                text  \n",
       "0  \\n\\nAttacks 83 yr old man - YouTube\\n\\n\\n\\n\\n\\...  \n",
       "1  \\nSource: 'It's making children cry': Police t...  \n",
       "2   MSNBC?s Matthews: Tea Party Racist by Using W...  \n",
       "3  \\n \\r\\n\\t\\t\\t\\r\\n\\t\\t\\tMay we still refer to t...  \n",
       "4  \\n \\nWe have become the animal farm, and some ...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_authors = pd.read_sql_query('''\n",
    "    SELECT DISTINCT author, thread_url FROM posts\n",
    "''', stormfront_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "authors_to_threads = defaultdict(list)\n",
    "\n",
    "for i, row in df_authors.iterrows():\n",
    "    authors_to_threads[row.author].append(row.thread_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15448,\n",
       " ['https://www.stormfront.org/forum/t988671',\n",
       "  'https://www.stormfront.org/forum/t1037525',\n",
       "  'https://www.stormfront.org/forum/t1000366',\n",
       "  'https://www.stormfront.org/forum/t988762',\n",
       "  'https://www.stormfront.org/forum/t988089',\n",
       "  'https://www.stormfront.org/forum/t899416',\n",
       "  'https://www.stormfront.org/forum/t894550'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(authors_to_threads), authors_to_threads['Dave Yorkshire'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "authors_to_ix = defaultdict(list)\n",
    "\n",
    "for author, threads in authors_to_threads.items():\n",
    "    for thread in threads:\n",
    "        ix = df_text[df_text.thread_url == thread].index.item()\n",
    "        authors_to_ix[author].append(ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..................................................................."
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "processed_text = []\n",
    "i = 0\n",
    "for doc in nlp.pipe(df_text.text, n_threads=8, batch_size=100):\n",
    "    if i % 1000 == 0:\n",
    "        sys.stdout.write('.')\n",
    "        sys.stdout.flush()\n",
    "    \n",
    "    ents = doc.ents\n",
    "    \n",
    "    t = [token.lemma_ for token in doc if token.is_alpha and not token.is_stop]\n",
    "    t.extend([str(entity) for entity in ents if len(entity) > 1])\n",
    "    \n",
    "    processed_text.append(t)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "dictionary = Dictionary(processed_text)\n",
    "\n",
    "# Remove rare and common tokens.\n",
    "# Filter out words that occur too frequently or too rarely.\n",
    "max_freq = 0.5\n",
    "min_wordcount = 30\n",
    "dictionary.filter_extremes(no_below=min_wordcount, no_above=max_freq)\n",
    "\n",
    "_ = dictionary[0]  # This sort of \"initializes\" dictionary.id2token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(doc) for doc in processed_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens: 23383\n",
      "Number of documents: 66147\n"
     ]
    }
   ],
   "source": [
    "print('Number of unique tokens: %d' % len(dictionary))\n",
    "print('Number of documents: %d' % len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.models import AuthorTopicModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-39a8f7517ff9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAuthorTopicModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_topics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid2word\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid2token\u001b[0m\u001b[0;34m,\u001b[0m                 \u001b[0mauthor2doc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mauthors_to_ix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma_threshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-10\u001b[0m\u001b[0;34m,\u001b[0m                 \u001b[0meval_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/sam/text-analytics/venv/local/lib/python3.5/site-packages/gensim-0.13.4.1-py3.5-linux-x86_64.egg/gensim/models/atmodel.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, corpus, num_topics, id2word, author2doc, doc2author, chunksize, passes, iterations, decay, offset, alpha, eta, update_every, eval_every, gamma_threshold, serialized, serialization_path, minimum_probability, random_state)\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mauthor2doc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdoc2author\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m             \u001b[0muse_numpy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatcher\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauthor2doc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc2author\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunks_as_numpy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_numpy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__str__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sam/text-analytics/venv/local/lib/python3.5/site-packages/gensim-0.13.4.1-py3.5-linux-x86_64.egg/gensim/models/atmodel.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, corpus, author2doc, doc2author, chunksize, decay, offset, passes, update_every, eval_every, iterations, gamma_threshold, chunks_as_numpy)\u001b[0m\n\u001b[1;32m    657\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mauthor2doc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# For all authors in input corpus.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mdoc_ids\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauthor2doc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# For all documents in total corpus.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 659\u001b[0;31m                     \u001b[0mtrain_corpus_idx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m             \u001b[0;31m# Make the list of training documents unique.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = AuthorTopicModel(corpus=corpus, num_topics=20, id2word=dictionary.id2token, \\\n",
    "                author2doc=authors_to_ix, chunksize=100, passes=100, gamma_threshold=1e-10, \\\n",
    "                eval_every=0, iterations=1, random_state=i)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
